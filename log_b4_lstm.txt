2025-12-31 16:32:13.238393: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-31 16:32:24.919622: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
======================================================================
BASELINE: EFFICIENTNET-B4 + SIMPLE LSTM
======================================================================
Device: cuda
Model: EfficientNet-B4
Input Size: 380�380
Feature Map: 12�12�1792
Num Regions: 144
Feature Dim: 1792
----------------------------------------------------------------------
Batch Size: 32
Epochs: 50
Learning Rate: 1e-04
----------------------------------------------------------------------
Embed Size: 512
Hidden Size: 512
----------------------------------------------------------------------
Attention: DISABLED
Scheduled Sampling: DISABLED
======================================================================
Loaded features for 8091 images (7965.1 MB)

Loading captions...
Loaded captions for 8091 images
Cleaning captions...
Building vocabulary (min_freq=1)...
Vocab size: 8369
Max caption length: 34
Rare words filtered: 0 (0.0% of tokens)
Train images: 6472, Test images: 1619
Train batches: 1012, Val batches: 253
Total params: 16,448,177, Trainable: 16,448,177
C:\Users\LAPTOP\anaconda3\envs\fer\Lib\site-packages\torch\optim\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(

======================================================================
STARTING TRAINING
======================================================================


Epoch 1/50 [Train]: 100%|##########| 1012/1012 [03:34<00:00,  4.71it/s, loss=5.0920]
Epoch 1/50 [Val]: 100%|##########| 253/253 [00:13<00:00, 18.62it/s, loss=5.1780]

Epoch 1/50
Train Loss: 5.8904 | Val Loss: 5.2025
LR: 0.000100
Model saved to .\content\clean_data_flickr8k\baseline_efficientnet_b4_simple_lstm.pth
New best model! Val Loss: 5.2025


Epoch 2/50 [Train]: 100%|##########| 1012/1012 [03:39<00:00,  4.61it/s, loss=5.2843]
Epoch 2/50 [Val]: 100%|##########| 253/253 [00:15<00:00, 15.81it/s, loss=4.7988]

Epoch 2/50
Train Loss: 5.1626 | Val Loss: 4.8775
LR: 0.000100
Model saved to .\content\clean_data_flickr8k\baseline_efficientnet_b4_simple_lstm.pth
New best model! Val Loss: 4.8775


Epoch 3/50 [Train]: 100%|##########| 1012/1012 [03:24<00:00,  4.94it/s, loss=4.4832]
Epoch 3/50 [Val]: 100%|##########| 253/253 [00:18<00:00, 13.60it/s, loss=4.6051]

Epoch 3/50
Train Loss: 4.8982 | Val Loss: 4.6893
LR: 0.000100
Model saved to .\content\clean_data_flickr8k\baseline_efficientnet_b4_simple_lstm.pth
New best model! Val Loss: 4.6893


Epoch 4/50 [Train]: 100%|##########| 1012/1012 [03:28<00:00,  4.85it/s, loss=4.5583]
Epoch 4/50 [Val]: 100%|##########| 253/253 [00:15<00:00, 16.27it/s, loss=4.4647]

Epoch 4/50
Train Loss: 4.7155 | Val Loss: 4.5535
LR: 0.000100
Model saved to .\content\clean_data_flickr8k\baseline_efficientnet_b4_simple_lstm.pth
New best model! Val Loss: 4.5535


Epoch 5/50 [Train]: 100%|##########| 1012/1012 [03:36<00:00,  4.67it/s, loss=4.3749]
Epoch 5/50 [Val]: 100%|##########| 253/253 [00:16<00:00, 15.68it/s, loss=4.3681]

Epoch 5/50
Train Loss: 4.5816 | Val Loss: 4.4621
LR: 0.000100
Model saved to .\content\clean_data_flickr8k\baseline_efficientnet_b4_simple_lstm.pth
New best model! Val Loss: 4.4621


Epoch 6/50 [Train]: 100%|##########| 1012/1012 [03:34<00:00,  4.73it/s, loss=4.0160]
Epoch 6/50 [Val]: 100%|##########| 253/253 [00:15<00:00, 16.80it/s, loss=4.3163]

Epoch 6/50
Train Loss: 4.4779 | Val Loss: 4.3970
LR: 0.000100
Model saved to .\content\clean_data_flickr8k\baseline_efficientnet_b4_simple_lstm.pth
New best model! Val Loss: 4.3970


Epoch 7/50 [Train]: 100%|##########| 1012/1012 [03:34<00:00,  4.72it/s, loss=4.2451]
Epoch 7/50 [Val]: 100%|##########| 253/253 [00:15<00:00, 16.49it/s, loss=4.2664]

Epoch 7/50
Train Loss: 4.3927 | Val Loss: 4.3465
LR: 0.000100
Model saved to .\content\clean_data_flickr8k\baseline_efficientnet_b4_simple_lstm.pth
New best model! Val Loss: 4.3465


Epoch 8/50 [Train]: 100%|##########| 1012/1012 [03:22<00:00,  4.99it/s, loss=4.1418]
Epoch 8/50 [Val]: 100%|##########| 253/253 [00:16<00:00, 15.24it/s, loss=4.2180]

Epoch 8/50
Train Loss: 4.3214 | Val Loss: 4.3137
LR: 0.000100
Model saved to .\content\clean_data_flickr8k\baseline_efficientnet_b4_simple_lstm.pth
New best model! Val Loss: 4.3137


Epoch 9/50 [Train]: 100%|##########| 1012/1012 [03:36<00:00,  4.68it/s, loss=4.6333]
Epoch 9/50 [Val]: 100%|##########| 253/253 [00:16<00:00, 15.06it/s, loss=4.2019]

Epoch 9/50
Train Loss: 4.2572 | Val Loss: 4.2818
LR: 0.000100
Model saved to .\content\clean_data_flickr8k\baseline_efficientnet_b4_simple_lstm.pth
New best model! Val Loss: 4.2818


Epoch 10/50 [Train]: 100%|##########| 1012/1012 [03:33<00:00,  4.73it/s, loss=4.3178]
Epoch 10/50 [Val]: 100%|##########| 253/253 [00:15<00:00, 16.59it/s, loss=4.1485]

Epoch 10/50
Train Loss: 4.2006 | Val Loss: 4.2628
LR: 0.000100
Model saved to .\content\clean_data_flickr8k\baseline_efficientnet_b4_simple_lstm.pth
New best model! Val Loss: 4.2628


Epoch 11/50 [Train]: 100%|##########| 1012/1012 [03:37<00:00,  4.65it/s, loss=4.6709]
Epoch 11/50 [Val]: 100%|##########| 253/253 [00:16<00:00, 15.27it/s, loss=4.1178]

Epoch 11/50
Train Loss: 4.1491 | Val Loss: 4.2355
LR: 0.000100
Model saved to .\content\clean_data_flickr8k\baseline_efficientnet_b4_simple_lstm.pth
New best model! Val Loss: 4.2355


Epoch 12/50 [Train]: 100%|##########| 1012/1012 [03:33<00:00,  4.73it/s, loss=4.3202]
Epoch 12/50 [Val]: 100%|##########| 253/253 [00:15<00:00, 16.13it/s, loss=4.0847]

Epoch 12/50
Train Loss: 4.1021 | Val Loss: 4.2194
LR: 0.000100
Model saved to .\content\clean_data_flickr8k\baseline_efficientnet_b4_simple_lstm.pth
New best model! Val Loss: 4.2194


Epoch 13/50 [Train]: 100%|##########| 1012/1012 [03:36<00:00,  4.68it/s, loss=3.9688]
Epoch 13/50 [Val]: 100%|##########| 253/253 [00:15<00:00, 16.67it/s, loss=4.0868]

Epoch 13/50
Train Loss: 4.0558 | Val Loss: 4.2059
LR: 0.000100
Model saved to .\content\clean_data_flickr8k\baseline_efficientnet_b4_simple_lstm.pth
New best model! Val Loss: 4.2059


Epoch 14/50 [Train]: 100%|##########| 1012/1012 [03:28<00:00,  4.85it/s, loss=4.3318]
Epoch 14/50 [Val]: 100%|##########| 253/253 [00:15<00:00, 16.64it/s, loss=4.0409]

Epoch 14/50
Train Loss: 4.0155 | Val Loss: 4.1888
LR: 0.000100
Model saved to .\content\clean_data_flickr8k\baseline_efficientnet_b4_simple_lstm.pth
New best model! Val Loss: 4.1888


Epoch 15/50 [Train]: 100%|##########| 1012/1012 [03:13<00:00,  5.24it/s, loss=4.4525]
Epoch 15/50 [Val]: 100%|##########| 253/253 [00:09<00:00, 26.37it/s, loss=4.0479]

Epoch 15/50
Train Loss: 3.9777 | Val Loss: 4.1819
LR: 0.000100
Model saved to .\content\clean_data_flickr8k\baseline_efficientnet_b4_simple_lstm.pth
New best model! Val Loss: 4.1819


Epoch 16/50 [Train]: 100%|##########| 1012/1012 [02:55<00:00,  5.78it/s, loss=4.0530]
Epoch 16/50 [Val]: 100%|##########| 253/253 [00:10<00:00, 25.25it/s, loss=4.0256]

Epoch 16/50
Train Loss: 3.9429 | Val Loss: 4.1798
LR: 0.000100
Model saved to .\content\clean_data_flickr8k\baseline_efficientnet_b4_simple_lstm.pth
New best model! Val Loss: 4.1798


Epoch 17/50 [Train]: 100%|##########| 1012/1012 [02:46<00:00,  6.07it/s, loss=4.0870]
Epoch 17/50 [Val]: 100%|##########| 253/253 [00:08<00:00, 28.87it/s, loss=4.0361]

Epoch 17/50
Train Loss: 3.9062 | Val Loss: 4.1718
LR: 0.000100
Model saved to .\content\clean_data_flickr8k\baseline_efficientnet_b4_simple_lstm.pth
New best model! Val Loss: 4.1718


Epoch 18/50 [Train]: 100%|##########| 1012/1012 [02:47<00:00,  6.05it/s, loss=3.6536]
Epoch 18/50 [Val]: 100%|##########| 253/253 [00:08<00:00, 28.57it/s, loss=4.0129]

Epoch 18/50
Train Loss: 3.8740 | Val Loss: 4.1647
LR: 0.000100
Model saved to .\content\clean_data_flickr8k\baseline_efficientnet_b4_simple_lstm.pth
New best model! Val Loss: 4.1647


Epoch 19/50 [Train]: 100%|##########| 1012/1012 [02:57<00:00,  5.69it/s, loss=3.6053]
Epoch 19/50 [Val]: 100%|##########| 253/253 [00:21<00:00, 11.77it/s, loss=4.0019]

Epoch 19/50
Train Loss: 3.8440 | Val Loss: 4.1627
LR: 0.000100
Model saved to .\content\clean_data_flickr8k\baseline_efficientnet_b4_simple_lstm.pth
New best model! Val Loss: 4.1627


Epoch 20/50 [Train]: 100%|##########| 1012/1012 [03:22<00:00,  5.00it/s, loss=3.9930]
Epoch 20/50 [Val]: 100%|##########| 253/253 [00:10<00:00, 23.11it/s, loss=3.9870]

Epoch 20/50
Train Loss: 3.8119 | Val Loss: 4.1535
LR: 0.000100
Model saved to .\content\clean_data_flickr8k\baseline_efficientnet_b4_simple_lstm.pth
New best model! Val Loss: 4.1535


Epoch 21/50 [Train]: 100%|##########| 1012/1012 [03:24<00:00,  4.95it/s, loss=3.5270]
Epoch 21/50 [Val]: 100%|##########| 253/253 [00:15<00:00, 16.12it/s, loss=3.9843]

Epoch 21/50
Train Loss: 3.7833 | Val Loss: 4.1650
LR: 0.000100
No improvement for 1 epoch(s)


Epoch 22/50 [Train]: 100%|##########| 1012/1012 [03:38<00:00,  4.64it/s, loss=4.0600]
Epoch 22/50 [Val]: 100%|##########| 253/253 [00:15<00:00, 16.24it/s, loss=3.9785]

Epoch 22/50
Train Loss: 3.7558 | Val Loss: 4.1588
LR: 0.000100
No improvement for 2 epoch(s)


Epoch 23/50 [Train]: 100%|##########| 1012/1012 [03:29<00:00,  4.83it/s, loss=3.6920]
Epoch 23/50 [Val]: 100%|##########| 253/253 [00:15<00:00, 16.20it/s, loss=4.0001]

Epoch 23/50
Train Loss: 3.7157 | Val Loss: 4.1669
LR: 0.000070
No improvement for 3 epoch(s)


Epoch 24/50 [Train]: 100%|##########| 1012/1012 [03:27<00:00,  4.88it/s, loss=3.7092]
Epoch 24/50 [Val]: 100%|##########| 253/253 [00:16<00:00, 15.30it/s, loss=3.9992]

Epoch 24/50
Train Loss: 3.6963 | Val Loss: 4.1624
LR: 0.000070
No improvement for 4 epoch(s)


Epoch 25/50 [Train]: 100%|##########| 1012/1012 [03:28<00:00,  4.86it/s, loss=3.2585]
Epoch 25/50 [Val]: 100%|##########| 253/253 [00:15<00:00, 16.80it/s, loss=3.9905]

Epoch 25/50
Train Loss: 3.6638 | Val Loss: 4.1634
LR: 0.000049
No improvement for 5 epoch(s)
Early stopping after 25 epochs

TRAINING COMPLETED


Training completed!
Best model saved at: .\content\clean_data_flickr8k\baseline_efficientnet_b4_simple_lstm.pth
C:\Users\LAPTOP\Documents\Code\Python\EfficientNet_Full_Flavor_Attention.py:899: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(paths['model'], map_location=Config.DEVICE)
Loaded best model from epoch 20 with val loss 4.1535


Research Evaluation: 100%|##########| 1619/1619 [02:04<00:00, 12.98it/s]

RESEARCH METRICS (BASELINE - NO ATTENTION)
==================================================
BLEU-1 (nltk): 0.5020
BLEU-2 (nltk): 0.3368
BLEU-3 (nltk): 0.2250
BLEU-4 (nltk): 0.1409
BLEU-1 (linear BP): 0.5007
BLEU-2 (linear BP): 0.3359
BLEU-3 (linear BP): 0.2244
BLEU-4 (linear BP): 0.1406
METEOR: 0.2958
Avg Pred Len: 7.5553
Avg Ref Len: 7.9636
Len Ratio: 0.9487
==================================================

============================================================
Image ID: 3004291093_35d6fd8548
ACTUAL CAPTIONS:
1. startseq man has bleached tips in his afro style hair and is on cellphone endseq
2. startseq man with black hair and blonde tips talks on his cellphone endseq
3. startseq man with crazy hair is sitting with his fingers in his ears endseq
4. startseq man with dreadlocks is plugging his ear to hear phone call endseq
5. startseq person with lots of hair and pacman jacket is making phone call endseq

PREDICTED (greedy):
two people are standing in front of christmas wall

PREDICTED (beam size=3):
group of people are posing for picture
============================================================


============================================================
Image ID: 3411579899_0f8ed09142
ACTUAL CAPTIONS:
1. startseq biker speeds down dirt path in the woods endseq
2. startseq kid with white shirt and helmet riding bike through the woods endseq
3. startseq cyclist riding down dirt path in the woods endseq
4. startseq mountain biking in forest during the early fall endseq
5. startseq the cyclist rides on wooded path endseq

PREDICTED (greedy):
person on bike is jumping over hill

PREDICTED (beam size=3):
person is riding bike through the woods
============================================================


============================================================
Image ID: 2629334536_11f2d49e05
ACTUAL CAPTIONS:
1. startseq baby in blue shirt is in swing endseq
2. startseq little blondeheaded child sits in the swing above the lush green grass endseq
3. startseq young girl plays on swing endseq
4. startseq young person wearing shorts and blue top is swinging in child protector seat endseq
5. startseq the little kid is swinging in the backyard endseq

PREDICTED (greedy):
little girl in pink shirt is swinging on swing

PREDICTED (beam size=3):
little girl swings on swing
============================================================


Done.
